<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="浏览器的模拟–Header属性有的时候，我们无法爬取一些网页，会出现403错误，因为这些网页为了防止被人恶意采集其信息所以进行了一些反爬虫的设置。可以设置一些Header信息，模拟成浏览器去访问这些网站，解决这种问题。 爬取CSDN博客的内容为例：123import urllib.requesturl = &quot;http://blog.csdn.net/weiwei_pig/article/detai">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/2017/12/17/python爬虫笔记（二）：利用模拟浏览器爬取网页/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="浏览器的模拟–Header属性有的时候，我们无法爬取一些网页，会出现403错误，因为这些网页为了防止被人恶意采集其信息所以进行了一些反爬虫的设置。可以设置一些Header信息，模拟成浏览器去访问这些网站，解决这种问题。 爬取CSDN博客的内容为例：123import urllib.requesturl = &quot;http://blog.csdn.net/weiwei_pig/article/detai">
<meta property="og:updated_time" content="2017-12-17T13:03:05.466Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
<meta name="twitter:description" content="浏览器的模拟–Header属性有的时候，我们无法爬取一些网页，会出现403错误，因为这些网页为了防止被人恶意采集其信息所以进行了一些反爬虫的设置。可以设置一些Header信息，模拟成浏览器去访问这些网站，解决这种问题。 爬取CSDN博客的内容为例：123import urllib.requesturl = &quot;http://blog.csdn.net/weiwei_pig/article/detai">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-python爬虫笔记（二）：利用模拟浏览器爬取网页" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/12/17/python爬虫笔记（二）：利用模拟浏览器爬取网页/" class="article-date">
  <time datetime="2017-12-17T10:46:00.269Z" itemprop="datePublished">2017-12-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="浏览器的模拟–Header属性"><a href="#浏览器的模拟–Header属性" class="headerlink" title="浏览器的模拟–Header属性"></a><center>浏览器的模拟–Header属性</center></h1><p>有的时候，我们无法爬取一些网页，会出现403错误，因为这些网页为了防止被人恶意采集其信息所以进行了一些反爬虫的设置。<br>可以设置一些Header信息，模拟成浏览器去访问这些网站，解决这种问题。</p>
<p>爬取CSDN博客的内容为例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">url = <span class="string">"http://blog.csdn.net/weiwei_pig/article/details/51178226"</span></span><br><span class="line">file = urllib.request.urlopen(url)</span><br></pre></td></tr></table></figure></p>
<p>此时会出现403异常，即禁止访问的错误，所以接下来我们需要让爬虫模拟成浏览器。模拟成浏览器可以设置User-Agent信息。通过我寻找到百度的User-agent直接使用：如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">User-Agent:Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36</span><br></pre></td></tr></table></figure></p>
<h1 id="方法一：使用build-opener-修改报头"><a href="#方法一：使用build-opener-修改报头" class="headerlink" title="方法一：使用build_opener()修改报头"></a>方法一：使用build_opener()修改报头</h1><p>由于urlopen()不支持一些HTTP的高级功能，所以，我们要修改报头，可以使用urllib.request.build_opener()进行，模拟浏览器的代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">url = <span class="string">"http://blog.csdn.net/weiwei_pig/article/details/51178226"</span></span><br><span class="line">headers = (<span class="string">"User-Agent"</span>,<span class="string">"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36"</span>)</span><br><span class="line">opener = urllib.request.build_opener()</span><br><span class="line">opener.addheaders = [headers]</span><br><span class="line"></span><br><span class="line">data = opener.open(url).read()</span><br></pre></td></tr></table></figure></p>
<p>爬取的内容可以写入文件：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fhandle = open(<span class="string">"e:/scrapy_data/3.html"</span>,<span class="string">"wb"</span>)</span><br><span class="line">fhandle.write(data)</span><br><span class="line">fhandle.close()</span><br></pre></td></tr></table></figure></p>
<h1 id="方法二：使用add-header-添加报头"><a href="#方法二：使用add-header-添加报头" class="headerlink" title="方法二：使用add_header()添加报头"></a>方法二：使用add_header()添加报头</h1><p>还可以利用urllib.request.Request()下的add_header()实现浏览器的模拟。代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">url = <span class="string">"http://blog.csdn.net/weiwei_pig/article/details/51178226"</span></span><br><span class="line">req = urllib.request.Request(url)</span><br><span class="line">req.add_header (<span class="string">"User-Agent"</span>,<span class="string">"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36"</span>)</span><br><span class="line">data = urllib.request.urlopen(url).read()</span><br></pre></td></tr></table></figure></p>
<h1 id="代理服务器的设置"><a href="#代理服务器的设置" class="headerlink" title="代理服务器的设置"></a>代理服务器的设置</h1><p>网上整理好的网址(代理服务器)[<a href="http://yum.iqianyue.com/proxy" target="_blank" rel="noopener">http://yum.iqianyue.com/proxy</a>] 中找到很多代理服务器地址。打开网页之后，尽量找到验证时间比较短的。此时，我们可以选择第二个代理IP地址223.153.131.205，对应端口808，完整的格式为：“网址：端口号”，即223.153.131.205：808<br>用了代理IP地址之后，编写程序如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">use_proxy</span><span class="params">(proxy_addr,url)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> urllib.request</span><br><span class="line">    proxy = urllib.request.ProxyHandler(&#123;<span class="string">'http'</span>:proxy_addr&#125;)</span><br><span class="line">    opener = urllib.request.build_opener(proxy,urllib.request.HTTPHandler)</span><br><span class="line">    urllib.request.install_opener(opener)</span><br><span class="line">    data = urllib.request.urlopen(url).read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line">proxy_addr = <span class="string">"223.153.131.205：808"</span></span><br><span class="line">data = use_proxy(proxy_addr,<span class="string">"http://www.baidu.xom"</span>)</span><br><span class="line">print(len(data))</span><br></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/12/17/python爬虫笔记（二）：利用模拟浏览器爬取网页/" data-id="cjbask2o30004bwu5mkv8zjl2" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/12/17/python爬虫笔记（二）：利用模拟浏览器爬取网页-1/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Neuer</strong>
      <div class="article-nav-title">
        
          python爬虫笔记（二）：利用模拟浏览器爬取网页
        
      </div>
    </a>
  
  
    <a href="/2017/12/17/python爬虫学习笔记(一)：open( )函数打开文件路径报错问题/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Älter</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/12/17/python爬虫笔记（二）：利用模拟浏览器爬取网页-1/">python爬虫笔记（二）：利用模拟浏览器爬取网页</a>
          </li>
        
          <li>
            <a href="/2017/12/17/python爬虫笔记（二）：利用模拟浏览器爬取网页/">(no title)</a>
          </li>
        
          <li>
            <a href="/2017/12/17/python爬虫学习笔记(一)：open( )函数打开文件路径报错问题/">(no title)</a>
          </li>
        
          <li>
            <a href="/2017/12/15/Markdown/">Markdown</a>
          </li>
        
          <li>
            <a href="/2017/12/14/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>