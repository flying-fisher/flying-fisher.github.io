<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title></title>
      <link href="/2018/03/23/%E6%97%A5%E5%B8%B8%E7%AC%94%E8%AE%B0/github%E4%B8%8A%E4%BC%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/03/23/%E6%97%A5%E5%B8%B8%E7%AC%94%E8%AE%B0/github%E4%B8%8A%E4%BC%A0%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<hr><p>title: github上传笔记</p><h2 id="date-2018-3-23-19：01：10"><a href="#date-2018-3-23-19：01：10" class="headerlink" title="date: 2018-3-23 19：01：10"></a>date: 2018-3-23 19：01：10</h2><h1 id="Github上传本地文档笔记"><a href="#Github上传本地文档笔记" class="headerlink" title="Github上传本地文档笔记"></a><center>Github上传本地文档笔记</center></h1><h2 id="分三步："><a href="#分三步：" class="headerlink" title="分三步："></a>分三步：</h2><ul><li>1.git add .(后面的点代表是要上传全部文件，如果上传一个文件，可以修改成文件名字)</li><li>2.git commit -m “说明上传的理由”</li><li>3.git push -u origin master(如果前面出现错误了，输入后者代码git pull –rebase origin master，然后再重新输入前面的代码)</li></ul>]]></content>
      
      
    </entry>
    
    <entry>
      <title>第一部分：爬虫概念，工具和HTTP</title>
      <link href="/2018/03/21/python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86/"/>
      <url>/2018/03/21/python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86/</url>
      <content type="html"><![CDATA[<h1 id="爬虫概念，工具和HTTP"><a href="#爬虫概念，工具和HTTP" class="headerlink" title="爬虫概念，工具和HTTP"></a>爬虫概念，工具和HTTP</h1><h2 id="1-什么是爬虫"><a href="#1-什么是爬虫" class="headerlink" title="1.什么是爬虫"></a>1.什么是爬虫</h2><ul><li>爬虫就是’模拟客户端（浏览器）发送网络请求’，获取响应，按照规则提取数据的程序</li><li>‘模拟客户端（浏览器）发送网络请求’: 照着浏览器发送一模一样的请求获取和浏览器一模一样的数据</li></ul><h2 id="2-爬虫的数据去哪了"><a href="#2-爬虫的数据去哪了" class="headerlink" title="2.爬虫的数据去哪了"></a>2.爬虫的数据去哪了</h2><ul><li>呈现出来：展现在网页上，或者是展现在app上</li><li>进行分析：从数据中寻找一些规律</li></ul><h2 id="3-需要的软件和环境"><a href="#3-需要的软件和环境" class="headerlink" title="3.需要的软件和环境"></a>3.需要的软件和环境</h2><ul><li>python3 </li><li>基础语法（字符串，列表，字典，判断和循环）</li><li>函数（函数的创建和调用）</li><li>面向对象（如何创建一个类，如何使用这个类）</li><li>工具IDE ：pycharm<ul><li>pycharm编辑器</li></ul></li><li>chrome浏览器<ul><li>分析网络请求用的</li></ul></li></ul><h2 id="4-浏览器的请求"><a href="#4-浏览器的请求" class="headerlink" title="4.浏览器的请求"></a>4.浏览器的请求</h2><ul><li><p>url</p><ul><li>在chrome中点击检查，点到network，</li><li>url = 请求的协议 + 网站的域名 + 资源的路径 + 参数</li></ul></li><li><p>浏览器请求url地址</p><ul><li>当前url对应的响应 + js + css + 图片 —&gt;&gt;elements中的内容</li></ul></li><li>爬虫请求url地址<ul><li>当前url对应的响应</li></ul></li><li><p>elements的内容和爬虫获取到的url地址的响应不同，爬虫中需要以当前url地址对应的响应为准提取数据</p></li><li><p>当前url地址对应的响应在哪里</p><ul><li>从network中找到当前的url地址，点击reponse</li><li>在页面上右键显示页面源码</li></ul></li></ul><h2 id="5-认识HTTP、HTTPS"><a href="#5-认识HTTP、HTTPS" class="headerlink" title="5.认识HTTP、HTTPS"></a>5.认识HTTP、HTTPS</h2><ul><li>HTTP：超文本传输协议<ul><li>以明文的形式传输</li><li>效率更高，不安全</li></ul></li><li><p>HTTPS:HTTP + SSL（安全套接字层）</p><ul><li>传输之前数据先加密，之后解密获取内容</li><li>效率较低，安全</li></ul></li><li><p>get请求和post请求的区别</p><ul><li>get请求没有请求体，post有，get请求把数据放到url地址中</li><li>post请求常用于登录注册，</li><li>post请求携带的数据量比get请求大，多，常用于传输大文本的时候</li></ul></li><li><p>HTTP协议请求</p><ul><li>1.请求行</li><li>2.请求头<ul><li>User-Agent:用户代理：对方服务器能够通过user-agent知道当前请求对方资源的是什么浏览器</li><li>如果我们需要模拟手机版的浏览器发送请求，就需要把user-agent改成手机版的</li><li>Cookie：用来存储用户信息的，每次请求会被携带上发送给对方的浏览器<ul><li>要获取登录后才能访问的页面</li><li>对方的服务器会通过cookie来判断我们是一个爬虫</li></ul></li></ul></li><li>3.请求体<ul><li>携带数据的</li><li>get请求没有请求体</li><li>post请求有请求体</li></ul></li></ul></li><li>HTTP协议之响应<ul><li>1.响应头<ul><li>Set-Cookie：对方服务器通过该字段设置cookie到本地</li></ul></li><li>2.响应体<ul><li>url地址对应的响应</li></ul></li></ul></li></ul>]]></content>
      
      
        <tags>
            
            <tag> 爬虫概念;HTTP; </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>第二部分：requests模块学习</title>
      <link href="/2018/03/21/python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86/"/>
      <url>/2018/03/21/python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0/%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86/</url>
      <content type="html"><![CDATA[<h1 id="requests模块的学习"><a href="#requests模块的学习" class="headerlink" title="requests模块的学习"></a>requests模块的学习</h1><h2 id="使用事前"><a href="#使用事前" class="headerlink" title="使用事前"></a>使用事前</h2><pre><code>-pip install requests</code></pre><h2 id="发送get-post请求，获取响应"><a href="#发送get-post请求，获取响应" class="headerlink" title="发送get,post请求，获取响应"></a>发送get,post请求，获取响应</h2><ul><li>response = requests.get(url) #发送get请求，请求url地址对应的响应</li><li>response = requests.post(url,data={请求体的字典}) #发送post请求</li></ul><h3 id="response-的方法"><a href="#response-的方法" class="headerlink" title="response 的方法"></a>response 的方法</h3><ul><li>response.text<ul><li>容易出现乱码，修改方式是前面加上response.encoding = ‘utf-8’</li></ul></li><li>response.content.decode()<ul><li>把响应的二进制字节流转化为str类型</li></ul></li><li>response.request.url  #发送请求的url地址</li><li>response.url  #response响应的url地址</li><li>response.request.headers  # 请求头</li><li>response.headers  #响应请求</li></ul><h3 id="获取网页源码的正确打开方式-通过下面三种方式一定可以获得相应的网页请求"><a href="#获取网页源码的正确打开方式-通过下面三种方式一定可以获得相应的网页请求" class="headerlink" title="获取网页源码的正确打开方式(通过下面三种方式一定可以获得相应的网页请求)"></a>获取网页源码的正确打开方式(通过下面三种方式一定可以获得相应的网页请求)</h3><ul><li>1.response.content.decode()</li><li>2.response.content.decode(‘gbk’)</li><li>3.response.text</li></ul><h2 id="发送带Header的请求-字典形式"><a href="#发送带Header的请求-字典形式" class="headerlink" title="发送带Header的请求(字典形式)"></a>发送带Header的请求(字典形式)</h2><ul><li>为了模拟浏览器，获取和浏览器一模一样的内容</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'</span> &#125;</span><br><span class="line"></span><br><span class="line">response = requests.get(url,headers = headers)</span><br></pre></td></tr></table></figure><h2 id="使用超时参数"><a href="#使用超时参数" class="headerlink" title="使用超时参数"></a>使用超时参数</h2><ul><li>response = requests.get(url,headers = headers,timeout=3)  #3秒内必须返回响应，否则会报错</li></ul><h3 id="retrying模块学习"><a href="#retrying模块学习" class="headerlink" title="retrying模块学习"></a>retrying模块学习</h3><ul><li>pip install retrying</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> retrying <span class="keyword">import</span> retry</span><br><span class="line"></span><br><span class="line"><span class="meta">@retry(stop_max_attempt_number = 3) #让被装饰的函数反复执行三次，三次全部报错才会报错，中间又一次正常，就可以正常进行</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun1</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">"this is func1"</span>)</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">"this is test error"</span>)</span><br></pre></td></tr></table></figure><h2 id="处理cookie相关的请求"><a href="#处理cookie相关的请求" class="headerlink" title="处理cookie相关的请求"></a>处理cookie相关的请求</h2><ul><li>人人网{“email”:”mr_mao_hacker@163.com”,<br>  “password”:”alarmchime”}</li><li><p>直接携带cookie请求url地址</p><ul><li><p>1.cookie放在headers中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">headers = &#123;<span class="string">"User-Agent"</span>:<span class="string">"......"</span>,<span class="string">"Cookie"</span>:<span class="string">"cookie 字符串"</span>&#125;</span><br></pre></td></tr></table></figure></li><li><p>2.cookie字典传给cookies参数</p><ul><li>requests.get(url,cookie=cookie_dict)</li></ul></li></ul></li><li>先发送post请求，获取cookie，带上cookie请求登录后的页面<ul><li>1.session = requests.session() #session具有的方法和requests一样</li><li>2.session.post(url,data,headers) # 服务器设置在本地的cookie会保存在session</li><li>3.session.get(url) # 会带上之前保存在session中的cookie，能够请求成功</li></ul></li></ul><h1 id="字典推导式"><a href="#字典推导式" class="headerlink" title="字典推导式"></a>字典推导式</h1><ul><li>可以把字符串转化为字典形式  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cookie = <span class="string">"BIDUPSID=D7F0CDF633A0261AC63033983555E4B8; PSTM=1502592197; BAIDUID=688900B6A843179BB4682296FC040DCC:FG=1; BDUSS=B3azBtSjg4VjNHajZsc3h6R01mfmwyaTZ2Ty1HaWhWWS1Ba1lnRGNGZjhsTTlaSUFBQUFBJCQAAAAAAAAAAAEAAABReqJYt9bJ7cr1MjAxNgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPwHqFn8B6hZej; H_PS_PSSID=1439_21109_17001_22159; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; BD_CK_SAM=1; PSINO=2; BDRCVFR[feWj1Vr5u3D]=mk3SLVN4HKm; BD_UPN=14314654; WWW_ST=1521449610205"</span></span><br><span class="line">cookie_dict = &#123;i.split(<span class="string">"="</span>)[<span class="number">0</span>]:i.split(<span class="string">"="</span>)[<span class="number">-1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> cookie.split(<span class="string">"; "</span>)&#125;</span><br><span class="line">print(cookie_dict)</span><br><span class="line">response = requests.get(url,headers = headers,cookies=cookie_dict)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"renren2.html"</span>,<span class="string">"w"</span>,encoding = <span class="string">"utf-8"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(response.content.decode())</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
        <tags>
            
            <tag> requests;retrying </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>浏览器的模拟--Header属性</title>
      <link href="/2017/12/17/%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/python%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%88%A9%E7%94%A8%E6%A8%A1%E6%8B%9F%E6%B5%8F%E8%A7%88%E5%99%A8%E7%88%AC%E5%8F%96%E7%BD%91%E9%A1%B5/"/>
      <url>/2017/12/17/%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/python%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%88%A9%E7%94%A8%E6%A8%A1%E6%8B%9F%E6%B5%8F%E8%A7%88%E5%99%A8%E7%88%AC%E5%8F%96%E7%BD%91%E9%A1%B5/</url>
      <content type="html"><![CDATA[<h1 id="浏览器的模拟–Header属性"><a href="#浏览器的模拟–Header属性" class="headerlink" title="浏览器的模拟–Header属性"></a><center>浏览器的模拟–Header属性</center></h1><p>有的时候，我们无法爬取一些网页，会出现403错误，因为这些网页为了防止被人恶意采集其信息所以进行了一些反爬虫的设置。<br>可以设置一些Header信息，模拟成浏览器去访问这些网站，解决这种问题。</p><p>爬取CSDN博客的内容为例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">url = <span class="string">"http://blog.csdn.net/weiwei_pig/article/details/51178226"</span></span><br><span class="line">file = urllib.request.urlopen(url)</span><br></pre></td></tr></table></figure></p><p>此时会出现403异常，即禁止访问的错误，所以接下来我们需要让爬虫模拟成浏览器。模拟成浏览器可以设置User-Agent信息。通过我寻找到百度的User-agent直接使用：如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">User-Agent:Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36</span><br></pre></td></tr></table></figure></p><h1 id="方法一：使用build-opener-修改报头"><a href="#方法一：使用build-opener-修改报头" class="headerlink" title="方法一：使用build_opener()修改报头"></a>方法一：使用build_opener()修改报头</h1><p>由于urlopen()不支持一些HTTP的高级功能，所以，我们要修改报头，可以使用urllib.request.build_opener()进行，模拟浏览器的代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">url = <span class="string">"http://blog.csdn.net/weiwei_pig/article/details/51178226"</span></span><br><span class="line">headers = (<span class="string">"User-Agent"</span>,<span class="string">"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36"</span>)</span><br><span class="line">opener = urllib.request.build_opener()</span><br><span class="line">opener.addheaders = [headers]</span><br><span class="line"></span><br><span class="line">data = opener.open(url).read()</span><br></pre></td></tr></table></figure></p><p>爬取的内容可以写入文件：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fhandle = open(<span class="string">"e:/scrapy_data/3.html"</span>,<span class="string">"wb"</span>)</span><br><span class="line">fhandle.write(data)</span><br><span class="line">fhandle.close()</span><br></pre></td></tr></table></figure></p><h1 id="方法二：使用add-header-添加报头"><a href="#方法二：使用add-header-添加报头" class="headerlink" title="方法二：使用add_header()添加报头"></a>方法二：使用add_header()添加报头</h1><p>还可以利用urllib.request.Request()下的add_header()实现浏览器的模拟。代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">url = <span class="string">"http://blog.csdn.net/weiwei_pig/article/details/51178226"</span></span><br><span class="line">req = urllib.request.Request(url)</span><br><span class="line">req.add_header (<span class="string">"User-Agent"</span>,<span class="string">"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36"</span>)</span><br><span class="line">data = urllib.request.urlopen(url).read()</span><br></pre></td></tr></table></figure></p><h1 id="代理服务器的设置"><a href="#代理服务器的设置" class="headerlink" title="代理服务器的设置"></a>代理服务器的设置</h1><p>网上整理好的网址(代理服务器)[<a href="http://yum.iqianyue.com/proxy" target="_blank" rel="noopener">http://yum.iqianyue.com/proxy</a>] 中找到很多代理服务器地址。打开网页之后，尽量找到验证时间比较短的。此时，我们可以选择第二个代理IP地址223.153.131.205，对应端口808，完整的格式为：“网址：端口号”，即223.153.131.205：808<br>用了代理IP地址之后，编写程序如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">use_proxy</span><span class="params">(proxy_addr,url)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> urllib.request</span><br><span class="line">    proxy = urllib.request.ProxyHandler(&#123;<span class="string">'http'</span>:proxy_addr&#125;)</span><br><span class="line">    opener = urllib.request.build_opener(proxy,urllib.request.HTTPHandler)</span><br><span class="line">    urllib.request.install_opener(opener)</span><br><span class="line">    data = urllib.request.urlopen(url).read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line">proxy_addr = <span class="string">"223.153.131.205：808"</span></span><br><span class="line">data = use_proxy(proxy_addr,<span class="string">"http://www.baidu.xom"</span>)</span><br><span class="line">print(len(data))</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
        <tags>
            
            <tag> 浏览器header </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python爬虫学习笔记(一)</title>
      <link href="/2017/12/17/%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%80)%EF%BC%9Aopen(%20)%E5%87%BD%E6%95%B0%E6%89%93%E5%BC%80%E6%96%87%E4%BB%B6%E8%B7%AF%E5%BE%84%E6%8A%A5%E9%94%99%E9%97%AE%E9%A2%98/"/>
      <url>/2017/12/17/%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%80)%EF%BC%9Aopen(%20)%E5%87%BD%E6%95%B0%E6%89%93%E5%BC%80%E6%96%87%E4%BB%B6%E8%B7%AF%E5%BE%84%E6%8A%A5%E9%94%99%E9%97%AE%E9%A2%98/</url>
      <content type="html"><![CDATA[<h1 id="python爬虫学习笔记-一-：open（）函数打开文件路径报错"><a href="#python爬虫学习笔记-一-：open（）函数打开文件路径报错" class="headerlink" title="python爬虫学习笔记(一)：open（）函数打开文件路径报错"></a><center>python爬虫学习笔记(一)：open（）函数打开文件路径报错<center></center></center></h1><h2 id="要以读文件的模式打开一个文件对象，使用Python内置的open-函数，传入文件名和标示符，标示符’r’表示读。"><a href="#要以读文件的模式打开一个文件对象，使用Python内置的open-函数，传入文件名和标示符，标示符’r’表示读。" class="headerlink" title="要以读文件的模式打开一个文件对象，使用Python内置的open()函数，传入文件名和标示符，标示符’r’表示读。"></a>要以读文件的模式打开一个文件对象，使用Python内置的open()函数，传入文件名和标示符，标示符’r’表示读。</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">file = open(<span class="string">'e:\scrapy_data\1.html'</span>,<span class="string">'wb'</span>)</span><br></pre></td></tr></table></figure><h2 id="对于初学python者，open（）函数很容易报错，并且不易被发现"><a href="#对于初学python者，open（）函数很容易报错，并且不易被发现" class="headerlink" title="对于初学python者，open（）函数很容易报错，并且不易被发现"></a>对于初学python者，open（）函数很容易报错，并且不易被发现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fhandle = open(<span class="string">"E:\scrapy_data\1.html"</span>,<span class="string">"wb"</span>)</span><br><span class="line">fhandle.write(data)</span><br><span class="line"></span><br><span class="line">OSError                                   Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input<span class="number">-24</span>-cb34cdc3c531&gt; <span class="keyword">in</span> &lt;module&gt;()</span><br><span class="line">----&gt; 1 fhandle = open("E:\scrapy_data\1.html","wb")</span><br><span class="line">      <span class="number">2</span> fhandle.write(data)</span><br><span class="line"></span><br><span class="line">OSError: [Errno <span class="number">22</span>] Invalid argument: <span class="string">'E:\\scrapy_data\x01.html'</span></span><br></pre></td></tr></table></figure><p>看着跟教程一模一样！只有斜杠的方向不一样，这里跟平常见得斜杠语法不一样，python语言里有一个转义字符的概念，需要在字符中使用特殊字符时，python用反斜杠’\’转义字符，所以错误案例中的’\’被用作转义，导致解释器解释文件路径错误。</p><ul><li>改正的方法有两种：<br>   第一、将’\’的方向反向为’/’，即文首正确的写法；<br>   第二、在含有转义符的字符串前加‘r’表示字符串内按原始含义解释，不做转义处理。（推荐！）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">file = open(<span class="string">r"e:\scrapy_data\1.html"</span>)</span><br></pre></td></tr></table></figure><h1 id="转义字符表"><a href="#转义字符表" class="headerlink" title="转义字符表"></a>转义字符表</h1><center><table><br>    <tr><br>        <td>转义字符</td><br>        <td>描述</td><br>    </tr><br>    <tr><br>        <td>(在行尾时)</td><br>        <td>续行符</td><br>    </tr><br>    <tr><br>        <td>\</td><br>        <td>反斜杠符号</td><br>    </tr><br>    <tr><br>        <td>\’</td><br>        <td>单引号</td><br>    </tr><br>    <tr><br>        <td>\”</td><br>        <td>双引号</td><br>    </tr><br>    <tr><br>        <td>\n</td><br>        <td>换行</td><br>    </tr><br>    <tr><br>        <td>\t</td><br>        <td>横向制表符</td><br>    </tr><br>    <tr><br>        <td>\v</td><br>        <td>纵向制表符</td><br>    </tr><br>    <tr><br>        <td>\r</td><br>        <td>回车</td><br>    </tr><br>    <tr><br>        <td>\f</td><br>        <td><br>换页</td><br>    </tr><br>    <tr><br>        <td>\oyy</td><br>        <td>八进制数，yy代表的字符，例如：\o12代表换行</td><br>    </tr><br></table><br></center><h1 id="open-函数标识符表"><a href="#open-函数标识符表" class="headerlink" title="open()函数标识符表"></a>open()函数标识符表</h1><ul><li>r 以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。</li><li>rb 以二进制格式打开一个文件用于只读。文件指针将会放在文件的开头。这是默认模式。</li><li>r+ 打开一个文件用于读写。文件指针将会放在文件的开头。</li><li>rb+ 以二进制格式打开一个文件用于读写。文件指针将会放在文件的开头。</li><li>w 打开一个文件只用于写入。如果该文件已存在则将其覆盖。如果该文件不存在，创建新文件。</li><li>wb 以二进制格式打开一个文件只用于写入。如果该文件已存在则将其覆盖。如果该文件不存在，创建新文件。</li><li>w+ 打开一个文件用于读写。如果该文件已存在则将其覆盖。如果该文件不存在，创建新文件。</li><li>wb+ 以二进制格式打开一个文件用于读写。如果该文件已存在则将其覆盖。如果该文件不存在，创建新文件。</li><li>a 打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。</li><li>ab 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。</li><li>a+ 打开一个文件用于读写。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。</li><li>ab+ 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。</li></ul>]]></content>
      
      
        <tags>
            
            <tag> open()函数 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>初识hexo博客</title>
      <link href="/2017/12/14/%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86hexo/hello-world/"/>
      <url>/2017/12/14/%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86hexo/hello-world/</url>
      <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
    </entry>
    
  
  
</search>
